### 2026-01-28
#### The Computational Case for Hypocrisy
[The Computational Case for Hypocrisy - by Aditya Kulkarni](https://thelivingfossils.substack.com/p/the-computational-case-for-hypocrisy) #evo-psych #evolution #psychology 

> Training massive AI models like Gemini or ChatGPT is an exercise in brute force. It costs hundreds of millions of dollars and requires server farms the size of industrial parks. The result of this process is a “Base Model”—a frozen, complex network of mathematical weights that “knows” how to predict the next word in a sentence.
> 
> Humans have an equivalent “Base Model,” too.
> 
> It resides in evolutionarily older decision systems that operate largely outside conscious processes. Just like an LLM, this biological base model was pre-trained on a massive dataset: millions of years of evolutionary trial and error. Its weights are heavily optimized for a specific set of survival outputs: _Consume high calories. Pursue mating opportunities. Dominate rivals._


> As AI researchers have discovered, it is almost impossible to subtract from a neural network. If you take a fully trained neural network and try to force it to [“unlearn”](https://arxiv.org/abs/2310.02238) a core concept—or aggressively “retrain” it on new, contradictory data—you trigger a phenomenon known as [Catastrophic Forgetting](https://www.sciencedirect.com/science/chapter/bookseries/abs/pii/S0079742108605368). Because knowledge in a neural network is distributed across billions of connections, you cannot simply isolate and delete a specific bad behavior without unraveling the rest of the system. If you force the model to unlearn “aggression,” you might accidentally degrade its ability to navigate terrain or recognize faces.

> When AI researchers want to “fine-tune” an AI model to learn a new, specific behavior, they often use a technique called Low-Rank Adaptation (LoRA).
> 
> Instead of melting down an AI model’s neural weights and recasting them, researchers have discovered that simply attaching a small, thin layer of _new_ parameters on top of the model allows you to change its behavior. It is a lightweight mask that sits over the heavy, deep machinery. This “Adapter Layer” intercepts the output of the frozen model and steers it in a new direction.
> 
> To change the behavior, you don’t touch the foundation. You build an addition.
> 
> Evolution likely arrived at the same architecture…


> Instead, evolution built a “LoRA Adapter”—the neocortical Press Secretary. This adapter doesn’t stop the impulse from firing; it layers a transparency over it. It translates the raw signal—“I want to eat this cake”—into the socially acceptable output: _“I am carbo-loading for a run.”_
> 
> The Press Secretary evolved because “re-training” the amygdala is practically impossible. It would be like reshooting an entire movie just to translate the dialogue into French. You don’t fly the actors back to the set; you just add dubbed audio. Hypocrisy is the adapter layer that allows a Paleolithic brain to operate in our modern civilization.