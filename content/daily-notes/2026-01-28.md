### 2026-01-28
#### The Computational Case for Hypocrisy
[The Computational Case for Hypocrisy - by Aditya Kulkarni](https://thelivingfossils.substack.com/p/the-computational-case-for-hypocrisy) #evo-psych #evolution #psychology 

> Training massive AI models like Gemini or ChatGPT is an exercise in brute force. It costs hundreds of millions of dollars and requires server farms the size of industrial parks. The result of this process is a “Base Model”—a frozen, complex network of mathematical weights that “knows” how to predict the next word in a sentence.
> 
> Humans have an equivalent “Base Model,” too.
> 
> It resides in evolutionarily older decision systems that operate largely outside conscious processes. Just like an LLM, this biological base model was pre-trained on a massive dataset: millions of years of evolutionary trial and error. Its weights are heavily optimized for a specific set of survival outputs: _Consume high calories. Pursue mating opportunities. Dominate rivals._


> As AI researchers have discovered, it is almost impossible to subtract from a neural network. If you take a fully trained neural network and try to force it to [“unlearn”](https://arxiv.org/abs/2310.02238) a core concept—or aggressively “retrain” it on new, contradictory data—you trigger a phenomenon known as [Catastrophic Forgetting](https://www.sciencedirect.com/science/chapter/bookseries/abs/pii/S0079742108605368). Because knowledge in a neural network is distributed across billions of connections, you cannot simply isolate and delete a specific bad behavior without unraveling the rest of the system. If you force the model to unlearn “aggression,” you might accidentally degrade its ability to navigate terrain or recognize faces.

